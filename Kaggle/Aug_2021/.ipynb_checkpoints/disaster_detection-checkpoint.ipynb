{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disaster detection from tweets\n",
    "This was a project from a Kaggle tutorial on Deep learning for NLP. The aim of the project is to predict whether a tweet is about an actual disaster or not based on the content of the tweet\n",
    "\n",
    "[Tutorial link](https://www.kaggle.com/philculliton/nlp-getting-started-tutorial)\n",
    "\n",
    "[LSA tutorial](http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/)\n",
    "\n",
    "[Count Vectorizer, TF-IDF vectorizer](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import sklearn\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-disaster tweet: I love fruits\n",
      "Ddisaster tweet: Forest fire near La Ronge Sask. Canada\n"
     ]
    }
   ],
   "source": [
    "''' Read data and look at examples of each type\n",
    "'''\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print('Non-disaster tweet: ', end='')\n",
    "print(train_df[train_df[\"target\"] == 0][\"text\"].values[1])\n",
    "\n",
    "print('Ddisaster tweet: ', end='')\n",
    "print(train_df[train_df[\"target\"] == 1][\"text\"].values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get statistics of the dataset\n",
    "* How many training example of each category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training dataset ======\n",
      "Disaster tweets = 3271, no disaster tweets = 4342 out of 7613 tweets\n",
      "Random assignment accuracy: 57.03\n",
      "\n",
      "If we just predicted every tweet as being disastrous, the f1-score is: 60.11\n",
      "So we should do better than this....\n"
     ]
    }
   ],
   "source": [
    "disaster = (train_df['target']==1).sum();\n",
    "no_disaster = (train_df['target']==0).sum();\n",
    "total = disaster + no_disaster\n",
    "\n",
    "print('===== Training dataset ======')\n",
    "print(f'Disaster tweets = {disaster}, no disaster tweets = {no_disaster} out of {total} tweets')\n",
    "print(f'Random assignment accuracy: {np.max((disaster, no_disaster))/total*100:0.2f}')\n",
    "\n",
    "y_true = train_df['target'];\n",
    "y_pred = np.ones_like(y_true);\n",
    "print('\\nIf we just predicted every tweet as being disastrous, the f1-score is:',end=\" \")\n",
    "print(f'{sklearn.metrics.f1_score(y_true,y_pred)*100:0.2f}')\n",
    "\n",
    "print('So we should do better than this....')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building vectors\n",
    "Let's first make a basic classifier where we use the words contained in the tweet to determine if the tweet is about a real disaster or not\n",
    "\n",
    "Below I report f1-scores for the following methods\n",
    "\n",
    "* Using scikit-learn's CountVectorizer to count the number of occurrences of every word and then classifying using a Ridge Regression\n",
    "* Using TF-IDF Vectorizer\n",
    "* Using a Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame();#df = pd.DataFrame(columns=['Method','f1-score'])\n",
    "clf = linear_model.RidgeClassifier()\n",
    "# clf = linear_model.Lasso(alpha=1e-6); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) Count Vectorizer + Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n",
    "test_vectors = count_vectorizer.transform(test_df[\"text\"])\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"].astype(int), cv=3, scoring=\"f1\")\n",
    "scores\n",
    "\n",
    "df_scores = df_scores.append({'Method':'CountVectorizer+Ridge','f1':np.mean(scores)},ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not much better than random assignment accuracy of 57%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) TF-IDF Vectorizer + Ridge Regression\n",
    "Count Vectorizer weighs each word equally, but there might be some words that are present often but do not provide much information about the content, e.g., 'a', 'the' etc. In order to account for that, we can use a different vectorizer called TF-IDF, where each terms's frequency $(tf)$ is multiplied by $idf$ or inverse-document frequency. If a word occurs in a lot of documents (here, tweets), then it's idf is low and vice-versa\n",
    "\n",
    "In short, the rarer a word, the higher is its TF-IDF score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = feature_extraction.text.TfidfVectorizer()\n",
    "train_vectors_tfidf = vectorizer.fit_transform(train_df[\"text\"])\n",
    "scores = model_selection.cross_val_score(clf, train_vectors_tfidf, train_df[\"target\"].astype(int), cv=3, scoring=\"f1\")\n",
    "scores\n",
    "df_scores = df_scores.append({'Method':'TfidVectorizer+Ridge','f1':np.mean(scores)},ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Latent Semantic Analysis (LSA)\n",
    "LSA is basically a version of TF-IDF with reduced dimensionality via SVD decomposition. It preserves the high variance features and removes the high frequency features.\n",
    "\n",
    "Interestingly, the performance is lower with LSA suggesting that the high frequency/low variance terms are important in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(100)\n",
    "train_vectors_tfidf_svd = svd.fit_transform(train_vectors_tfidf)\n",
    "scores = model_selection.cross_val_score(clf, train_vectors_tfidf_svd, train_df[\"target\"].astype(int), cv=3, scoring=\"f1\")\n",
    "scores\n",
    "\n",
    "df_scores = df_scores.append({'Method':'LSA+Ridge','f1':np.mean(scores)},ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=21637, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "idx_sort_tgt = np.argsort(train_df['target'])\n",
    "sort_values = train_df['target'][idx_sort_tgt].values;\n",
    "sort_train = train_vectors[idx_sort_tgt,:].toarray()\n",
    "\n",
    "scores = model_selection.cross_val_score(model, sort_train, sort_values, cv=3,scoring=\"f1\")\n",
    "scores\n",
    "\n",
    "df_scores = df_scores.append({'Method':'MLP','f1':np.mean(scores)},ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
