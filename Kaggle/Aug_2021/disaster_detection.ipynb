{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disaster detection from tweets\n",
    "This was a project from a Kaggle tutorial on Deep learning for NLP. The aim of the project is to predict whether a tweet is about an actual disaster or not based on the content of the tweet\n",
    "\n",
    "[Tutorial link](https://www.kaggle.com/philculliton/nlp-getting-started-tutorial)\n",
    "\n",
    "[LSA tutorial](http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/)\n",
    "\n",
    "[Count Vectorizer, TF-IDF vectorizer](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "\n",
    "import sklearn\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-disaster tweet: I love fruits\n",
      "Ddisaster tweet: Forest fire near La Ronge Sask. Canada\n"
     ]
    }
   ],
   "source": [
    "''' Read data and look at examples of each type\n",
    "'''\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print('Non-disaster tweet: ', end='')\n",
    "print(train_df[train_df[\"target\"] == 0][\"text\"].values[1])\n",
    "\n",
    "print('Ddisaster tweet: ', end='')\n",
    "print(train_df[train_df[\"target\"] == 1][\"text\"].values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get statistics of the dataset\n",
    "* How many training example of each category?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Training dataset ======\n",
      "Disaster tweets = 3271, no disaster tweets = 4342 out of 7613 tweets\n",
      "Random assignment accuracy: 57.03\n",
      "\n",
      "If we just predicted every tweet as being disastrous, the f1-score is: 60.11\n",
      "So we should do better than this....\n"
     ]
    }
   ],
   "source": [
    "disaster = (train_df['target']==1).sum();\n",
    "no_disaster = (train_df['target']==0).sum();\n",
    "total = disaster + no_disaster\n",
    "\n",
    "print('===== Training dataset ======')\n",
    "print(f'Disaster tweets = {disaster}, no disaster tweets = {no_disaster} out of {total} tweets')\n",
    "print(f'Random assignment accuracy: {np.max((disaster, no_disaster))/total*100:0.2f}')\n",
    "\n",
    "y_true = train_df['target'];\n",
    "y_pred = np.ones_like(y_true);\n",
    "print('\\nIf we just predicted every tweet as being disastrous, the f1-score is:',end=\" \")\n",
    "print(f'{sklearn.metrics.f1_score(y_true,y_pred)*100:0.2f}')\n",
    "\n",
    "print('So we should do better than this....')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building vectors\n",
    "Let's first make a basic classifier where we use the words contained in the tweet to determine if the tweet is about a real disaster or not\n",
    "\n",
    "Below I report f1-scores for the following methods\n",
    "\n",
    "* Using scikit-learn's CountVectorizer to count the number of occurrences of every word and then classifying using a Ridge Regression\n",
    "* Using TF-IDF Vectorizer\n",
    "* Using a Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make dicts of all classifiers and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Dict of classifiers\n",
    "'''\n",
    "clf = linear_model.RidgeClassifier()\n",
    "# clf = linear_model.Lasso(alpha=1e-6); \n",
    "clf_knn = KNeighborsClassifier(n_neighbors=5, algorithm='brute')\n",
    "clf_log = LogisticRegression(penalty='l2')\n",
    "clf_svm = svm.SVC(gamma=0.1, C=2000);\n",
    "random_forest = RandomForestClassifier(n_estimators=100,min_samples_leaf=3, max_features=50)\n",
    "boosted_tree = AdaBoostClassifier(n_estimators=100)\n",
    "elastic_net = LogisticRegression(penalty='elasticnet',solver = 'saga', l1_ratio = 0.5)\n",
    "\n",
    "clf_dict ={'Elastic Net': elastic_net,'Ridge':clf, 'KNN':clf_knn, 'Log Reg':clf_log, 'SVM': clf_svm, \n",
    "           'Random Forest': random_forest,'Boosted tree': boosted_tree};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Dict of features\n",
    "'''\n",
    "# Count vectorizer\n",
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n",
    "\n",
    "# TF-IDF\n",
    "vectorizer = feature_extraction.text.TfidfVectorizer()\n",
    "train_vectors_tfidf = vectorizer.fit_transform(train_df[\"text\"])\n",
    "\n",
    "# LSA\n",
    "svd = TruncatedSVD(100)\n",
    "train_vectors_tfidf_svd = svd.fit_transform(train_vectors_tfidf)\n",
    "\n",
    "feat = {'Count Vectorizer':train_vectors, \n",
    "       'TF IDF': train_vectors_tfidf,\n",
    "       'LSA': train_vectors_tfidf_svd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-parameter search for SVM. Skip this for calculating scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = np.zeros((10,12))\n",
    "for ig, g in enumerate(np.logspace(-9,9,10)):\n",
    "    for iC, C in enumerate(np.logspace(-2, 10,12)):\n",
    "        scores = model_selection.cross_val_score(svm.SVC(C=C, gamma=g), train_vectors, \n",
    "                                                 train_df[\"target\"].astype(int),cv=3, scoring='f1')\n",
    "        ss[ig,iC] = scores.mean();\n",
    "        \n",
    "plt.imshow(ss);\n",
    "plt.xticks(np.arange(0,12), np.logspace(-2, 10,12),rotation=60); \n",
    "plt.yticks(np.arange(0,10), np.logspace(-9,9,10));\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameter search for Random Forest. Skip this for calculating final scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_samples_leaf = np.array([1,3,5,10,20,50]);\n",
    "max_features = np.array([10,15,20,30,50]);\n",
    "\n",
    "score_random_forest = np.zeros((len(min_samples_leaf), len(max_features)))\n",
    "for isample, ns in enumerate(min_samples_leaf):\n",
    "    for ifeature, nf in enumerate(max_features):\n",
    "        scores = model_selection.cross_val_score(RandomForestClassifier(n_estimators=100,min_samples_leaf=ns, max_features=nf),\n",
    "                                                train_vectors, train_df['target'].astype(int), cv=3, scoring='f1');\n",
    "        \n",
    "        score_random_forest[isample, ifeature] = scores.mean();\n",
    "plt.imshow(score_random_forest);\n",
    "plt.xticks(np.arange(0,len(max_features)), max_features, rotation=60); plt.xlabel('Max features');\n",
    "plt.yticks(np.arange(0,len(min_samples_leaf)), min_samples_leaf); plt.ylabel('Min samples leaf');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's calculate f-1 scores for each classifier with each feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame();\n",
    "for (key, item) in clf_dict.items():\n",
    "    for (key_f, item_f) in feat.items():\n",
    "        scores = model_selection.cross_val_score(item, item_f, \n",
    "                                                 train_df[\"target\"].astype(int),cv=3, scoring='f1')\n",
    "        df_scores = df_scores.append({'Features':key_f,'Classifier':key,\n",
    "                                  'f1':np.mean(scores)},ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not much better than random assignment accuracy of 57%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) TF-IDF Vectorizer + Ridge Regression\n",
    "Count Vectorizer weighs each word equally, but there might be some words that are present often but do not provide much information about the content, e.g., 'a', 'the' etc. In order to account for that, we can use a different vectorizer called TF-IDF, where each terms's frequency $(tf)$ is multiplied by $idf$ or inverse-document frequency. If a word occurs in a lot of documents (here, tweets), then it's idf is low and vice-versa\n",
    "\n",
    "In short, the rarer a word, the higher is its TF-IDF score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3) Latent Semantic Analysis (LSA)\n",
    "LSA is basically a version of TF-IDF with reduced dimensionality via SVD decomposition. It preserves the high variance features and removes the high frequency features.\n",
    "\n",
    "Interestingly, the performance is lower with LSA suggesting that the high frequency/low variance terms are important in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Features</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>Count Vectorizer</td>\n",
       "      <td>0.628939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>TF IDF</td>\n",
       "      <td>0.593253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.627579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>Count Vectorizer</td>\n",
       "      <td>0.608069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>TF IDF</td>\n",
       "      <td>0.643725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.634295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNN</td>\n",
       "      <td>Count Vectorizer</td>\n",
       "      <td>0.159711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNN</td>\n",
       "      <td>TF IDF</td>\n",
       "      <td>0.612973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNN</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.522100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Log Reg</td>\n",
       "      <td>Count Vectorizer</td>\n",
       "      <td>0.646067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Log Reg</td>\n",
       "      <td>TF IDF</td>\n",
       "      <td>0.632608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Log Reg</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.617410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Count Vectorizer</td>\n",
       "      <td>0.617608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SVM</td>\n",
       "      <td>TF IDF</td>\n",
       "      <td>0.625463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>SVM</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.653677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Count Vectorizer</td>\n",
       "      <td>0.385424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>TF IDF</td>\n",
       "      <td>0.400306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.612755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Boosted tree</td>\n",
       "      <td>Count Vectorizer</td>\n",
       "      <td>0.549200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Boosted tree</td>\n",
       "      <td>TF IDF</td>\n",
       "      <td>0.537344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Boosted tree</td>\n",
       "      <td>LSA</td>\n",
       "      <td>0.604001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Classifier          Features        f1\n",
       "0     Elastic Net  Count Vectorizer  0.628939\n",
       "1     Elastic Net            TF IDF  0.593253\n",
       "2     Elastic Net               LSA  0.627579\n",
       "3           Ridge  Count Vectorizer  0.608069\n",
       "4           Ridge            TF IDF  0.643725\n",
       "5           Ridge               LSA  0.634295\n",
       "6             KNN  Count Vectorizer  0.159711\n",
       "7             KNN            TF IDF  0.612973\n",
       "8             KNN               LSA  0.522100\n",
       "9         Log Reg  Count Vectorizer  0.646067\n",
       "10        Log Reg            TF IDF  0.632608\n",
       "11        Log Reg               LSA  0.617410\n",
       "12            SVM  Count Vectorizer  0.617608\n",
       "13            SVM            TF IDF  0.625463\n",
       "14            SVM               LSA  0.653677\n",
       "15  Random Forest  Count Vectorizer  0.385424\n",
       "16  Random Forest            TF IDF  0.400306\n",
       "17  Random Forest               LSA  0.612755\n",
       "18   Boosted tree  Count Vectorizer  0.549200\n",
       "19   Boosted tree            TF IDF  0.537344\n",
       "20   Boosted tree               LSA  0.604001"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm.fit(train_vectors_tfidf_svd, train_df['target'].astype(int))\n",
    "\n",
    "vectorizer_test = feature_extraction.text.TfidfVectorizer()\n",
    "test_vectors_tfidf = vectorizer.fit_transform(test_df[\"text\"])\n",
    "svd = TruncatedSVD(100)\n",
    "test_vectors_tfidf_svd = svd.fit_transform(test_vectors_tfidf)\n",
    "\n",
    "y_pred = clf_svm.predict(test_vectors_tfidf_svd);\n",
    "\n",
    "df_test = pd.DataFrame(test_df['id'])\n",
    "df_test['target'] = y_pred;\n",
    "df_test.to_csv('Predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I try to use an (bi)LSTM for classification. These worked very well for movie genre classification in the IMDB dataset in my other notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data, datasets\n",
    "import nltk\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length=50\n",
    "TEXT = data.Field(sequential=True,\n",
    "                    tokenize=nltk.word_tokenize,\n",
    "                    lower=True,\n",
    "                    include_lengths=True,\n",
    "                    batch_first=True,\n",
    "                    fix_length=sentence_length)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {'text': ('t', TEXT), 'target': ('l', LABEL)}\n",
    "train_data = data.TabularDataset.splits(\n",
    "                            path = '',\n",
    "                            train = 'train.csv',\n",
    "                            format = 'csv',\n",
    "                            fields = fields\n",
    ")\n",
    "train_data = train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=2021; batch_size=32\n",
    "TEXT.build_vocab(train_data)\n",
    "train_data, valid_data = train_data.split(split_ratio=0.7,\n",
    "                                            random_state=random.seed(seed))\n",
    "train_iter, valid_iter = data.BucketIterator.splits((train_data, valid_data),\n",
    "                                                                  batch_size=batch_size, sort_key=lambda x: len(x.text),\n",
    "                                                                  repeat=False, shuffle=True)\n",
    "vocab_size = len(TEXT.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.example.Example at 0x1a9eb58c18>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_from_tutorial(model, device, train_iter, valid_iter, epochs, learning_rate):\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  train_loss, validation_loss = [], []\n",
    "  train_acc, validation_acc = [], []\n",
    "\n",
    "  for epoch in tqdm(range(epochs),position=0,leave=True):\n",
    "    #train\n",
    "    model.train()\n",
    "    running_loss = 0.\n",
    "    correct, total = 0, 0\n",
    "    steps = 0\n",
    "\n",
    "    for idx, batch in enumerate(train_iter):\n",
    "      text = batch.text[0]\n",
    "      # print(type(text), text.shape)\n",
    "      target = batch.label\n",
    "      target = torch.autograd.Variable(target).long()\n",
    "      text, target = text.to(device), target.to(device)\n",
    "\n",
    "      # add micro for coding training loop\n",
    "      optimizer.zero_grad()\n",
    "      output = model(text)\n",
    "\n",
    "      loss = criterion(output, target)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      steps += 1\n",
    "      running_loss += loss.item()\n",
    "\n",
    "      # get accuracy\n",
    "      _, predicted = torch.max(output, 1)\n",
    "      total += target.size(0)\n",
    "      correct += (predicted == target).sum().item()\n",
    "\n",
    "    train_loss.append(running_loss/len(train_iter))\n",
    "    train_acc.append(correct/total)\n",
    "\n",
    "#     print(f'Epoch: {epoch + 1}, '\n",
    "#           f'Training Loss: {running_loss/len(train_iter):.4f}, '\n",
    "#           f'Training Accuracy: {100*correct/total: .2f}%')\n",
    "\n",
    "    # evaluate on validation data\n",
    "    model.eval()\n",
    "    running_loss = 0.\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for idx, batch in enumerate(valid_iter):\n",
    "        text = batch.text[0]\n",
    "        target = batch.label\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        text, target = text.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(text)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # get accuracy\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "    validation_loss.append(running_loss/len(valid_iter))\n",
    "    validation_acc.append(correct/total)\n",
    "\n",
    "#     print (f'Validation Loss: {running_loss/len(valid_iter):.4f}, '\n",
    "#            f'Validation Accuracy: {100*correct/total: .2f}%')\n",
    "\n",
    "  return train_loss, train_acc, validation_loss, validation_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LabelField' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-25a76df251c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m                                                                     \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                                                                     \u001b[0mvalid_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                                                                     30,0.0003)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-650db94f9fa1>\u001b[0m in \u001b[0;36mtrain_from_tutorial\u001b[0;34m(model, device, train_iter, valid_iter, epochs, learning_rate)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m       \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;31m# print(type(text), text.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchtext/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchtext/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \"\"\"\n\u001b[1;32m    236\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchtext/data/field.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LabelField' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,layers,output_size,hidden_size,vocab_size,embed_size):\n",
    "        super(LSTM,self).__init__();\n",
    "        self.n_layers=layers;\n",
    "        self.output_size=output_size;\n",
    "        self.hidden_size=hidden_size;\n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size,embed_size);\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=self.n_layers)\n",
    "        self.fc = nn.Linear(self.n_layers*self.hidden_size, self.output_size);\n",
    "        \n",
    "    def forward(self,input_sentences):\n",
    "        inp = self.word_embeddings(input_sentences).permute(1,0,2);\n",
    "        hidden = (torch.randn(self.n_layers, inp.shape[1], self.hidden_size),\n",
    "        torch.randn(self.n_layers, inp.shape[1], self.hidden_size))\n",
    "        \n",
    "        inp = self.dropout(inp);\n",
    "        output, hidden = self.lstm(inp, hidden);\n",
    "        \n",
    "        h_n = hidden[0].permute(1, 0, 2)\n",
    "        h_n = h_n.contiguous().view(h_n.shape[0], -1)\n",
    "\n",
    "        logits = self.fc(h_n)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "sampleLSTM = LSTM(2, 2, 50, vocab_size, 100)\n",
    "sampleLSTM\n",
    "\n",
    "bilstm_train_loss, bilstm_train_acc, bilstm_validation_loss, bilstm_validation_acc = train_from_tutorial(sampleLSTM,\n",
    "                                                                                                         'cpu',\n",
    "                                                                    train_iter,\n",
    "                                                                    valid_iter,\n",
    "                                                                    30,0.0003)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BucketIterator' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-280747bb91f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'BucketIterator' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "train_iter.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.42466887, 0.29617193])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1000, input_dim=21637, activation='relu'))\n",
    "    model.add(Dense(20, input_dim=1000, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "idx_sort_tgt = np.argsort(train_df['target'])\n",
    "sort_values = train_df['target'][idx_sort_tgt].values;\n",
    "sort_train = train_vectors[idx_sort_tgt,:].toarray()\n",
    "\n",
    "scores = model_selection.cross_val_score(model, sort_train, sort_values, cv=3,scoring=\"f1\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = df_scores.append({'Classifier':'MLP','Features':'Count Vectorizer','f1':np.mean(scores)},ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
