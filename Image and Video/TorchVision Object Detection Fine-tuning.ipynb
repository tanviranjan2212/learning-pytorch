{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Make Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a DataLoader for this dataset\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    # each loader class needs 3 fnx: __init__, __len, __getitem__\n",
    "    def __init(self, root, transforms):\n",
    "        self.root = root;\n",
    "        self.transforms = transforms;\n",
    "        \n",
    "        # load png files and corresponding masks, sort to make sure they aligned\n",
    "        self.images = list(sorted(os.listdir(os.path.join(root, 'PNGImages'))));\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root,'PedMasks'))));\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem(self, idx):\n",
    "        # get image and mask from the idx\n",
    "        \n",
    "        image_path = os.path.join(self.root,'PNGImages',self.images[idx])\n",
    "        image = Image.open(image_path).convert('RGB');\n",
    "        \n",
    "        mask_path = os.path.join(self.root,'PedMasks',self.masks[idx])\n",
    "        # don't convert mask to RGB since each color correponds to a different instance\n",
    "        mask = Image.open(mask_path)\n",
    "        mask = np.array(mask)\n",
    "        \n",
    "        #instances are encoded as different colors and the first is background, so remove it\n",
    "        obj_ids = np.unique(mask); obj_ids = obj_ids[1:];\n",
    "        \n",
    "        #split color coded masks into a set of binary masks\n",
    "        masks = mask==obj_ids[:,None,None]\n",
    "        \n",
    "        #get bounding box coordinates for each mask\n",
    "        #x-axis is dim 1, y-axis is dim 0\n",
    "        num_objs = len(obj_ids);\n",
    "        boxes=[];\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i]);\n",
    "            xmin, xmax = np.min(pos[1]), np.max(pos[1]);\n",
    "            ymin, ymax = np.min(pos[0]), np.max(pos[0]);\n",
    "            boxes.append([xmin, ymin, xmax, ymax]);\n",
    "            \n",
    "        #of course then convert everything to a tensor\n",
    "        boxes = torch.as_tensor(boxes,dtype=torch.float32)\n",
    "        \n",
    "        #there is only one class\n",
    "        labels = torch.ones((num_objs,),dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        \n",
    "        image_id = torch.as_tensor([idx])\n",
    "        \n",
    "        #area of masks = change in x * change in y\n",
    "        area = (boxes[:,3]-boxes[:,1])*(boxes[:,2]-boxes[:,4])\n",
    "        \n",
    "        #[optional] - but suppose there is no crowd\n",
    "        iscrowd = torch.zeros((num_objs,),dtype=torch.int64);\n",
    "        \n",
    "        #make target dataframe\n",
    "        target = {}\n",
    "        target['boxes']=boxes; target['labels']=labels; target['masks']=masks\n",
    "        target['image_id']=image_id; target['area']=area; target['iscrowd']=iscrowd;\n",
    "        \n",
    "        if self.tranforms is not None:\n",
    "            self.transforms(image, target)\n",
    "            \n",
    "        return image, target\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are two methods of training, in addition to training a new model\n",
    "1. Fine-tune an already trained model (i.e., only adjust the last parameters)\n",
    "2. Change the backbone of an existing model\n",
    "\n",
    "#### Starting with fine-tuning an already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sgmllib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8a2ceb3d00a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtransforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transforms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msafe_html\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msafe_html\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbodyfinder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# modules = [\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     'st',             # zopish\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     'rest',           # docutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transforms/safe_html.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msgmllib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGMLParser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSGMLParseError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcgi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mescape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msafeToInt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sgmllib'"
     ]
    }
   ],
   "source": [
    "import transforms as T\n",
    "def get_transform(train):\n",
    "    transforms = [];\n",
    "    transforms.append(T.ToTensor());\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return (T.Compose(transforms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use built-in engines and utils for easy evaluation\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "def main():\n",
    "    device = torch.device('cpu');\n",
    "    \n",
    "    num_classes = 2; #only 2 classes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
