{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP with Python and Machine Learning\n",
    "### Chapters 1-2: NLP Basics and Data Cleaning\n",
    "[Tutorial link here](https://www.linkedin.com/learning/nlp-with-python-for-machine-learning-essential-training/nltk-setup-and-overview?autoAdvance=true&autoSkip=true&autoplay=true&resume=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "# dir(nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What can you do with nltk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLP Basics: Reading in text data and cleaning it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ham\\tI've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\\nspam\\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\nham\\tNah I don't think he goes to usf, he lives around here though\\nham\\tEven my brother is not like to speak with me. They treat me like aid\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawData = open('Ex_Files_NLP_Python_ML_EssT/Exercise Files/Ch01/01_03/Start/SMSSpamCollection.tsv').read()\n",
    "\n",
    "rawData[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data reading and 1st pass on processing steps:\n",
    "* Segment into sentences\n",
    "* Make text and label lists\n",
    "* Convert to dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ham',\n",
       " \"I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.\",\n",
       " 'spam',\n",
       " \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       " 'ham']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedData=rawData.replace('\\t','\\n').split('\\n')\n",
    "parsedData[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5571\n",
      "5570\n",
      "['ham', 'ham', 'ham', 'ham', '']\n",
      "5570\n"
     ]
    }
   ],
   "source": [
    "labelList = parsedData[0::2]\n",
    "textList = parsedData[1::2]\n",
    "\n",
    "print(len(labelList))\n",
    "print(len(textList))\n",
    "print(labelList[-5:]); #last string is empty, so remove it\n",
    "\n",
    "labelList = labelList[:-1]\n",
    "print(len(labelList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  I've been searching for the right words to tha...\n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "3   ham  Even my brother is not like to speak with me. ...\n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_corpus = pd.DataFrame(\n",
    "{'label':labelList,\n",
    "'text': textList})\n",
    "\n",
    "full_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or I could've just read this from a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Ex_Files_NLP_Python_ML_EssT/Exercise Files/Ch01/01_03/Start/SMSSpamCollection.tsv',\n",
    "                     sep='\\t',header=None)\n",
    "\n",
    "dataset.head()\n",
    "dataset.columns = ['label','text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the dataset\n",
    "* Size and shape\n",
    "* Count of ham/spam\n",
    "* Missing data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5568, 2)\n",
      "Count(ham) vs count(spam): 4822 vs 746\n",
      "Missing label count: 0\n"
     ]
    }
   ],
   "source": [
    "print(dataset.shape)\n",
    "\n",
    "\n",
    "nham, nspam = len(dataset[dataset['label']=='ham']), len(dataset[dataset['label']=='spam'])\n",
    "\n",
    "print(f'Count(ham) vs count(spam): {nham} vs {nspam}')\n",
    "\n",
    "nmiss = dataset['label'].isna().sum()\n",
    "print(f'Missing label count: {nmiss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside: Using regular expressions\n",
    "* tokenization\n",
    "* replacement\n",
    "\n",
    "The `\\s` and `\\w` tokens correspond to spaces and words respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'made-up', 'string', 'to', 'try', 'different', 'regex', 'methods']\n",
      "['This', 'is', 'a', 'made-up', 'string', 'to', 'try', 'different', 'regex', 'methods']\n",
      "================================================\n",
      "['This', '', '', '', 'is', 'a', 'made-up', '', '', '', '', '', 'string', 'to', 'try', 'different', 'regex.', '', '', 'methods']\n",
      "['This', '', '', '', 'is', 'a', 'made-up', '', '', '', '', '', 'string', 'to', 'try', 'different', 'regex.', '', '', 'methods']\n",
      "['This', 'is', 'a', 'made-up', 'string', 'to', 'try', 'different', 'regex.', 'methods']\n",
      "================================================\n",
      "['This-is-a-really-messy<<<<<-string']\n",
      "['This-is-a-really-messy<<<<<-string']\n",
      "['This', 'is', 'a', 'really', 'messy', 'string']\n",
      "The same can be done with findall() instead of split()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I follow PEP8 Python Style guidelines'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Tokenization\n",
    "str1 = 'This is a made-up string to try different regex methods'\n",
    "str2 = 'This    is a made-up      string to try different regex.   methods'\n",
    "str3 = 'This-is-a-really-messy<<<<<-string'\n",
    "\n",
    "print(str1.split(' '));\n",
    "print(re.split('\\s', str1))\n",
    "print('================================================')\n",
    "\n",
    "print(str2.split(' '))\n",
    "print(re.split('\\s', str2))\n",
    "print(re.split('\\s+', str2))\n",
    "print('================================================')\n",
    "\n",
    "print(str3.split(' '))\n",
    "print(re.split('\\s+', str3))\n",
    "print(re.split('\\W+', str3))\n",
    "\n",
    "print('The same can be done with findall() instead of split()')\n",
    "\n",
    "# Replacement\n",
    "\n",
    "pep7 = 'I follow PEP7 guidelines'\n",
    "pep8 = 'I follow PEP8 guidelines'\n",
    "peep8 = 'I follow PEEP8 guidelines'\n",
    "#I want to replace all data of type 'PEP#' to 'PEP8 Python Style'\n",
    "\n",
    "re.sub('[A-Z]+[0-9]+','PEP8 Python Style',pep7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning pipeline\n",
    "\n",
    "* Tokenize text stream to words\n",
    "    * Clean tokenized data by removing stop words\n",
    "* Vectorize: Convert to numeric form\n",
    "* Fit ML model and predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing\n",
    "* Remove punctuation\n",
    "* Tokenization\n",
    "* Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>remove_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>[Ive, been, searching, for, the, right, words,...</td>\n",
       "      <td>[Ive, searching, right, words, thank, breather...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[Nah, I, dont, think, goes, usf, lives, around...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>[Even, my, brother, is, not, like, to, speak, ...</td>\n",
       "      <td>[Even, brother, like, speak, They, treat, like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]</td>\n",
       "      <td>[I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  [Ive, been, searching, for, the, right, words,...   \n",
       "1  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "2  [Nah, I, dont, think, he, goes, to, usf, he, l...   \n",
       "3  [Even, my, brother, is, not, like, to, speak, ...   \n",
       "4         [I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]   \n",
       "\n",
       "                                         remove_stop  \n",
       "0  [Ive, searching, right, words, thank, breather...  \n",
       "1  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...  \n",
       "2  [Nah, I, dont, think, goes, usf, lives, around...  \n",
       "3  [Even, brother, like, speak, They, treat, like...  \n",
       "4         [I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to remove punctuation\n",
    "def remove_punct(str_text):\n",
    "    str_no_punct = \"\".join([char for char in str_text if char not in string.punctuation])\n",
    "    return str_no_punct\n",
    "\n",
    "# Function to tokenize string into words\n",
    "def tokenize(str_text):\n",
    "    tok_text = re.split('\\W+', str_text)\n",
    "    return tok_text\n",
    "\n",
    "# Function to remove stop words from tokenized data\n",
    "def remove_stopwords(tokenized_list):\n",
    "    rem_sw_text = [word for word in tokenized_list if word not in stopwords]\n",
    "    return rem_sw_text\n",
    "\n",
    "# \n",
    "tst_string = 'This is. a line. with? punctuation!'\n",
    "dataset['text_clean'] = dataset['text'].apply(lambda x: remove_punct(x))\n",
    "dataset['text_clean'] = dataset['text_clean'].apply(lambda x: tokenize(x))\n",
    "dataset['remove_stop'] = dataset['text_clean'].apply(lambda x: remove_stopwords(x))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pickle('Spam_ch2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>remove_stop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>[Ive, been, searching, for, the, right, words,...</td>\n",
       "      <td>[Ive, searching, right, words, thank, breather...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[Nah, I, dont, think, goes, usf, lives, around...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>[Even, my, brother, is, not, like, to, speak, ...</td>\n",
       "      <td>[Even, brother, like, speak, They, treat, like...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]</td>\n",
       "      <td>[I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  \\\n",
       "0   ham  I've been searching for the right words to tha...   \n",
       "1  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "2   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "3   ham  Even my brother is not like to speak with me. ...   \n",
       "4   ham                I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  [Ive, been, searching, for, the, right, words,...   \n",
       "1  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "2  [Nah, I, dont, think, he, goes, to, usf, he, l...   \n",
       "3  [Even, my, brother, is, not, like, to, speak, ...   \n",
       "4         [I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]   \n",
       "\n",
       "                                         remove_stop  \n",
       "0  [Ive, searching, right, words, thank, breather...  \n",
       "1  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...  \n",
       "2  [Nah, I, dont, think, goes, usf, lives, around...  \n",
       "3  [Even, brother, like, speak, They, treat, like...  \n",
       "4         [I, HAVE, A, DATE, ON, SUNDAY, WITH, WILL]  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset.iloc[0]['text_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
